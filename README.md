The focus of this experiment was to train a language model to generate text character-by-character. For this task, an [English name corpus](https://data.world/nkrishnaswami/us-ssa-baby-names-national) was chosen from Data World which contains a large number of baby names. Considering the longer inputs, LSTM model was incorporated to generate the names. In this case, each of the input and output tokens is a character. Moreover, the language model outputs a conditional probability distribution over the character set. According to this generated probability distribution, the next token was picked using [Top-K sampler](https://huggingface.co/blog/how-to-generate#top-k-sampling). The codebase **(PyTorch)** can be found [here](https://github.com/AbuUbaida/nameGen-lstm).
